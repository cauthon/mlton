SimdOverview
=============
== Introduction to SIMD ==
== Introduction to SIMD in MLton ==
Simd operations are implemented in MLton as a series of high level signatures
and structures which are implemented by either a hardware or software backend.
The externally visable structures and signatures were designed to be mostly
independent of the internal implementation. That being said the only currently
available hardware backend is for amd64, and so the design of the front end was
infulenced largely by the avaiable amd64 simd instructions. The way in which
Simd instructions was implemented in MLton was infulenced by the implementation
of Simd intrinsics in many C and C++ compiliers as well as by the
implementation of Simd instructions in the Glascow Haskell Complier. Most
implementations of Simd instructions are fairly low level, and while that is
somewhat true for this, an attempt was made construct an interface similar to
other primitive sml types and hopefully integrate simd instructions into the
language in a more natural way.
== Implementation ==
Simd operations in MLton can be broadly generalized into two groups, operations
on <:SimdRealStructures:Simd Real:> types and <:SimdWordStructures:Simd Word:>
types. The naming convention for simd types is Simd<X>_Word/Real<Y>, indicating
a type of X bits with elements of Y bits each, for example a 128 bit simd type
for single precision floating point numbers would be Simd128_Real32. This
convention was adopted in order to allow scaling with larger simd types and to
fit with the sml model of using Real/Word<X> for different sizes of types. Simd
types are primitive types, but currently there is no way to specify a literal
simd value and so simd values must be loaded from other types. There are
several ways of loading a simd value present for each simd type. For a simd
type with N elements, the fromArray function loads the first N elements and the
fromArrayOffset function loads N elements starting at an index i. The
fromScalar function loads a given value into the lowest bytes of a simd value
and sets the rest to 0, and the fromScalarFill functions loads a given value
into each element of a simd type. To get from a simd type back to another sml
type there are the functions toArray which stores the simd value into the first
N indices of a given array and toScalar which returns the lowest element from
the simd value. For Simd Word types all loading and storing functions are
available in versions for integers and words.  Other than loading and storing
simd values behave similiary to any other sml primitive type, other than some
exceptions noted below. 
// May need to add more here
== Basic Usage Tips ==
While simd types were designed to work much like other primitive sml types
there are still a few differences to keep in mind when working with simd
types. First is Alignment, at least on x86 and amd64 simd values must be loaded
from a location aligned to the length of the simd type. So a 128 bit simd type
must be aligned to a 16 byte boundry and a 256 bit type to a 32 bit
boundary. Loading from unaligned adresses is allowed but results in noticably
slower performance. Foth the most part alignment is taken care of internally
so long as the proper alignment switch is given. The one case where alignment
must be taken into concideration is when using the toArrayOffset function, it
is possible to load from any array index without error, but in general
precaucion should be taken to assure that the index given lies on an aligned
adress to insure the best performance from simd instructions. Loading and
storing simd values is by far the most expensive part of using simd
instructions, and so loading and storing should be mininized as much as
possible. The key tool in minimizing memory access is the shuffle operation,
shuffle gives the ability to manipulate indivdal elements of each simd
type, with indexing starting at 0. For simd real values the shuffle function
takes two simd values and 4 word8 values. The word8 values select which
elements to place in the result, for a simd real/word64 value the first word8
selects from the first value and the second from the second value, and the
third and fourth are not used. For simd real/word32 the first two word8s select
from the first value and the last two from the second. Word8 and Word16 shuffle
operations are more complex, for details see <:SimdWordStructures:>. 

There are a few gotchas with using simd types to look out for. Firstly there is
no way to do arathmatic operations on words with carry, as this would require
keeping track of multiple carry bits. What is provided is saturated arithmatic,
where a mininum or maximum value will remain unchanged if subtraced from or
multiplied/added to respectively. This makes the most sense for graphical
programs which are where simd word instructions are mostly used. Secondly is
that comparisons do not return booleans naturally, they return a bit mask of 1s
for true and 0s for false. It is planned to add in convience functions to
return tuples of booleans, but for now comparisons only return bitmasks.

== Usage and Performance Tips ==
A general use case of Simd instructions is to vectorize loops, taking what
would be a normal loop over an array and using simd instructions in the body of
the loop to increase performance.
[source,sml]
----
fun SimdLoop(a:elt array,b:elt array) =
    let
       val len = Array.length(a)

       if len <> Array.length(b) then raise Subscript else 
       val dstArray = Array.array(len,Array.sub(a,0))
       (* this loop will do most of the work using simd instructions*)
       fun loop(i:int) =
       if i > len then i
       else 
          let
             val simdA = fromArrayOffset(a,i)
             val simdB = fromArrayOffset(b,i)
             (* Body of loop goes here *)
             val simdC = (* Result of Simd operations
                          * ex. add(simdA,simdB) *)
             val _ = toArrayOffset(dstArray,i,simdC)
           in
              loop(i+elements)
           end
       (* do calculations for excess elements that won't fill a simd value*)
       fun cleanUp(j:int) =
       if j = len then dstArray
       else
         let
            val scalarA = Array.sub(a,j)
            val scalarB = Array.sub(b,j)
            (* Body of loop here *)
            val scalarC = (* Result of loop
                           * ex. scalarA + scalarB *)
            val _ = Array.update(dstArray,j,scalarC)
         end
   in
      cleanUp(loop(0)-elements)
   end
----

This example shows a common setup for using simd instructions, as is obvious 
looking at it simdimd instructions are noticably more complicated to use that
simple scalar instructions. Looking at the example there are two main reasons
for the complexity, the first is the loading and storing of simd values, which
has already been adressed, the second is the necessity for the second scalar
loop. Vectorizing a loop with simd instrcutions is much like loop unrolling,
where the loop body is expanded to perform multiple iterations of the loop in
one iteration of the unrolled loop. This brings causes a problem with
vectorized loops as with unrolled loops, dealing with excess elements that will
not fit cleanly into a single iteration. For example if the above loop was
performed on two Real32 arrays of 15 elements each the simd loop could only
handle the first 12 elements, leaving 3 extra elements to be processed. It is
for this reason that the additonal loop is required, in actual code there is an
option of forcing all input arrays to have lengths that are a multiple of the
number of elements in a simd value, but this severly limits the flexiblity of
the resulting program.

A key consideration in deciding weather or not to use simd instructions is the
size of the input, while simd instructions can cause a significant increase in
performance this is not always worth the additonal programming complexity. It
is best to use simd instructions on loops that are used with large inputs or
repeated often, as this will result in the most noticable performance gains.


Common functional programming patterns can be enhanced using simd operations,
for example here is a sample implementation of a fold function on a simd148_word32
type.
[source,sml]
----
fun simdFold(arr:word array,f:simdWord*simdWord->simdWord)
    let
      val len=Array.length(arr)
      val acc=fromArray(arr)
      fun loop(acc,cnt)=
        if cnt > len then
           let
              val acc=f(shuffle(acc,acc,(0w3,0w2,0w1,0w0)),acc)
              val acc=f(shuffle(acc,acc,(0w2,0w3,0w0,0w1)),acc)
           in
              toScalar(acc)
           end
        else loop(f(acc,fromArrayOffset(arr,cnt)),cnt+elements)
      in
      loop(acc,4)
      end
----
A few things off note, shuffling is used to permute the elements and reduce
the set of 4 simd elements to a single scalar, which is then extracted as the
answer. This implementation relies on the function being used to accumulate
being assoitative and communtive, however if this is the case, simple addition
or multiplication for instance, this will run much faster than the scalar
version.
This raises an important point for simd floating point calculations. As no
floating point operations are assoicative care must be taken with vectorizing
floating point operations if accuracy is important. However if a little
accuracy can be sacrificed a notable increase in speed can be had in using
vectorized floating point calculations
movss   (%rdx), %xmm0
movss   (%rcx), %xmm3
unpcklps        %xmm0, %xmm3
movss   (%rdi), %xmm1
movss   (%rsi), %xmm2
unpcklps        %xmm1, %xmm2
movaps  %xmm3, %xmm0
movlhps %xmm2, %xmm0
